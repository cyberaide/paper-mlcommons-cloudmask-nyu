
\documentclass[sigplan,screen]{acmart}


\input{cloudmask-report-format}

\newcommand{\REVISION}{8. Jul. 2023, revised 8 Dec. 2023}

\acmConference[MLCommons Science Working Group Report]{MLCommons Cloud Masking Benchmark with Early Stopage}{\REVISION}{}

%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}

%%\acmSubmissionID{123-A56-BU3}

%%\citestyle{acmauthoryear}

\begin{document}



\newcommand{\TITLE}{MLCommons Cloud Masking Benchmark with Early Stopping}


\title[\TITLE]{MLCommons Cloud Masking Benchmark \\with Early Stopping \\ {\normalsize Version 1.0}}

\author{Varshitha Chennamsetti}
\affiliation{%
  \institution{New York University}
  \streetaddress{Center for Data Science\\
60 5th Ave}
  \city{New York}
  \state{NY}
  \postcode{10011}
  \country{USA}
}

\author{Gregor von Laszewski}
\email{laszewski@gmail.com}
\orcid{0000-0001-9558-179X}
\authornote{MLCommons authorized submitting author}
\affiliation{%
  \institution{University of Virginia}
  \streetaddress{Biocomplexity Institute\\
Town Center Four\\
994 Research Park Boulevard}
  \city{Charlottesville}
  \state{VA}
  \postcode{22911}
  \country{USA}
}

\author{Ruochen Gu}
\affiliation{%
  \institution{New York University}
  \streetaddress{Center for Data Science\\
60 5th Ave}
  \city{New York}
  \state{NY}
  \postcode{10011}
  \country{USA}
}

\author{Laiba Mehnaz}
\affiliation{%
  \institution{New York University}
  \streetaddress{Center for Data Science\\
60 5th Ave}
  \city{New York}
  \state{NY}
  \postcode{10011}
  \country{USA}
}



\author{Juri Papay}
\email{juri.papay@stfc.ac.uk}
\affiliation{%
  \institution{Rutherford Appleton Laboratory}
  \streetaddress{Harwell Campus}
  \city{Didcot}
  \postcode{OX11 0QX}
  \country{UK}
}

\author{Samuel Jackson}
\email{samuel.jackson@stfc.ac.uk}
\affiliation{%
  \institution{Rutherford Appleton Laboratory}
  \streetaddress{Harwell Campus}
  \city{Didcot}
  \postcode{OX11 0QX}
  \country{UK}
}


\author{Jeyan Thiyagalingam}
\email{t.jeyan@stfc.ac.uk}
\affiliation{%
  \institution{Rutherford Appleton Laboratory}
  \streetaddress{Harwell Campus}
  \city{Didcot}
  \postcode{OX11 0QX}
  \country{UK}
}

\author{Sergey V. Samsonau}
\affiliation{%
  \institution{New York University}
  \streetaddress{Center for Data Science\\
60 5th Ave}
  \city{New York}
  \state{NY}
  \postcode{10011}
  \country{USA}
}

\author{Geoffrey C. Fox}
\email{gcfexchange@gmail.com}
\affiliation{%
  \institution{University of Virginia}
  \streetaddress{Biocomplexity Institute\\
Town Center Four\\
994 Research Park Boulevard}
  \city{Charlottesville}
  \state{VA}
  \postcode{22911}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Chennamsetti, von Laszewski, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

In this paper, we report work performed for MLCommons Science Working Group on the cloud masking benchmark. MLCommons is a consortium that develops and maintains several scientific benchmarks that aim to  benefit developments in AI. The benchmarks are conducted on the High Performance Computing (HPC) Clutsters  of New York University and University of Virginia, as well as a commodity desktop. We provide a description of the cloud masking benchmark, as well as a summary of our submission to MLCommons on the benchmark experiment we conducted. It includes a modification to the reference implementation of the cloud masking benchmark enabling early stopping. This benchmark is executed on the NYU HPC through a custom batch script that runs the various experiments through the batch queuing system, while allowing for variation on the number of epochs trained. Our submission includes the modified code, a custom batch script to modify epochs, documentation\footnote{Disclaimer: the documentation from NYU no longer works due to a system upgrade at NYU. A transition to singularity was recommended by the NYU system staff. This has been independently worked on by Ruochen Gu. However, as the accounts on Greene expired, this work could not be completed and was therefore not documented in this paper. For availability of portable code, please contact G. von Laszewski.}, and the benchmark results. We report the highest accuracy (scientific metric) and the average time taken (performance metric) for training and inference that was achieved on NYU HPC Greene. We also provide a comparison of the compute capabilities between different systems by running the benchmark for one epoch. Our submission can be found in a Globus repository that is accessible to MLCommons Science Working Group. 


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010405.10010432.10010437</concept_id>
<concept_desc>Applied computing~Earth and atmospheric sciences</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
<concept_desc>Computing methodologies~Image representations</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003359.10003360</concept_id>
<concept_desc>Information systems~Test collections</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010583.10010737.10010749</concept_id>
<concept_desc>Hardware~Testing with distributed and parallel systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Earth and atmospheric sciences}
\ccsdesc[500]{Computing methodologies~Image representations}
\ccsdesc[500]{Information systems~Test collections}
\ccsdesc[500]{Hardware~Testing with distributed and parallel systems}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{cloud masking, cloudmesh, datasets, MLCommons, benchmark}


\received[Version from]{\REVISION}

\begin{comment}
\begin{center}
{\huge\bf \TITLE}
\end{center}

% \listoftodos

\tableofcontents
\listoffigures
\listoftables
\end{comment}

\clearpage

\maketitle


\section{Introduction}


\nocite{las-2023-ai-workflow} % add to avoid running bibtex multiple times

Although artificial intelligence (AI) emerges as a powerful technology capable of transforming many aspects of life, several scientific niches/fields have yet to fully benefit from the advances in AI. Hence, there is a need for an initiative to increase awareness and push forward innovation in scientific fields where the potential for AI is huge but underexplored. 
MLCommons \cite{www-mlcommons-research}  is one such community effort that promotes AI for Science via benchmarking. MLCommons Science Working Group \cite{Thiyagalingam2022AIBF} has so far developed four scientific benchmarks in four varying fields: atmospheric sciences, solid-state physics, healthcare, and earthquake forecasting. In this paper, we present our submission to the cloud masking science benchmark in the atmospheric sciences domain. 

The objective of this benchmark \cite{Thiyagalingam2022AIBF} is to identify cloud pixels given a satellite image (image segmentation). European Space Agency (ESA) \cite{www-sentinel92} has deployed a series of satellites to monitor the global environment. Amongst these satellites, Sentinel-3 has been made operational to focus on, among other missions, ocean surface topography and sea and land surface temperature (SST/LST). The process of the retrieval of SST and LST normally begins with cloud screening/masking, followed by the actual temperature estimation of the pixels that do not contain clouds. Cloud masking is performed since the presence of clouds can distort accurate estimations of temperature, making it a crucial step in the process of SST and LST estimation. 

Several methods have been used for this task ranging from rule-based \cite{Saunders1986AnAS,Saunders1988AnIM,Merchant2005ProbabilisticPB, Zhu2012ObjectbasedCA} to deep learning \cite{Li2019DeepLB,Domnich2021KappaMaskAC,Yan2018CloudAC,WIELAND2019111203,JEPPESEN2019247}. Some of the rule-based techniques have been threshold tests \cite{Saunders1986AnAS,Saunders1988AnIM} and Bayesian masking \cite{Merchant2005ProbabilisticPB}. Bayesian masking uses Bayes' theorem and prior meteorology information to generate probabilities for each pixel whether it contains cloud or not. Generalizing the process from a pixel, a cloud mask can then be generated from a satellite image. On the other hand, deep learning methods \cite{Li2019DeepLB,Domnich2021KappaMaskAC,Yan2018CloudAC,WIELAND2019111203,JEPPESEN2019247} use computer vision models and treat the cloud masking task as that of image segmentation. The most widely used model is U-Net \cite{Ronneberger2015UNetCN}, which is pre-trained and designed for such a task, especially for large images.


In this paper, we make our submission to the MLCommons Science Working group using the cloud masking benchmark. We modified the reference implementation by introducing early stopping. As for data, we use a selected set of satellite images from Sentinel-3. We report the scientific metric (accuracy) and performance metric 
(scalability, training and inference time)
for this benchmark using the NYU HPC Greene \cite{www-greene}. As part of this work, we have also done many in-depth experiments at UVA HPC Rivanna. Rivanna results are submitted separately and a much more  comprehensive analysis found with the same hyperparameter settings as used in this benchmark.

\section{MLCommons}

Scientific benchmarking is different from standard AI/ML benchmarking, particularly, their data differs in terms of both its volume and type \cite{las-2023-mlcommons-edu-eq}. The former requires heavy computing due to the I/O impact of its large-scale datasets, which leads to the use of supercomputers \cite{Farrell2021MLPerfHA}. MLCommons is a consortium that overlooks several scientific benchmarks in various scientific domains that can benefit from developments in AI. One of the working groups in MLCommons is the Science Working Group \cite{Thiyagalingam2022AIBF}, which maintains four different benchmarks at the time of writing this paper. A benchmark specifies a scientific task, provides the data for training and testing purposes, and defines a metric that needs to be reported by the contributors. In our case, the cloud mask benchmark provides 180GB worth of satellite image data, the task of which being cloud masking. The scientific metric is the classification accuracy of satellite images (i.e. the percentage of pixels classified correctly), and the performance metric is the scalability over the number of GPUs and the time taken for training and inference. 
For each benchmark, there is a reference implementation provided by the Science Working Group, which is what we are using for this benchmark in this paper.

\section{Related Work}

From \cite{las-2023-cloudmask-related} we quote the general motivation and related research for cloud masking: ``Cloud masking is a crucial task that is well-motivated for meteorology and its applications in environmental and atmospheric sciences. Its goal is, given satellite images, to accurately generate cloud masks that identify each pixel in images as cloud or clear sky.

Since last century, several methods have been developed for cloud masking, ranging from rule-based techniques \cite{Saunders1986AnAS,Saunders1988AnIM,Merchant2005ProbabilisticPB, Zhu2012ObjectbasedCA} to modern deep learning approaches \cite{Li2019DeepLB,Domnich2021KappaMaskAC,Yan2018CloudAC,WIELAND2019111203,JEPPESEN2019247}. Among the more traditional, rule-based techniques, two popular methodologies have been threshold cloud screening \cite{Saunders1986AnAS,Saunders1988AnIM} and Bayesian cloud masking \cite{Merchant2005ProbabilisticPB}. 

Threshold screening methods consist of several threshold tests where spectral and spatial properties of satellite images are compared with those ranges that are believed to indicate the presence of a clear sky pixel. And those other pixels that are not labeled as clear sky are then flagged as cloudy. This school of methodologies were widely used from the late 1980s to the early 2000s \cite{Merchant2005ProbabilisticPB}. 

The gradual transition away from threshold screening methods was due to its long-criticized limitations: firstly, threshold settings rely heavily on domain expertise about indicators of cloudiness that may not be objective, which also makes later modification and updates difficult; secondly, thresholds provide users little flexibility in the trade-off between coverage and accuracy; third, threshold tests do not make use of all available prior information. These shortcomings of threshold screening methods are improved by later developed Bayesian methods \cite{Merchant2005ProbabilisticPB}.

The Bayesian approach applies Bayes' theorem on prior meteorology information to deduce for each pixel the probability of containing cloud or clear sky, and thereafter generating a cloud mask as output. As a result, these Bayesian approaches are fully probabilistic and make good use of prior information. Compared to threshold tests, Bayesian methods achieve better accuracy in predicting pixels' cloudiness, offering generality and conceptual clarity in its approach, and enhancing maintainability and adaptability largely \cite{Merchant2005ProbabilisticPB}. 

More recently, the rising popularity of deep learning has led to the use of CNNs for generating cloud masks. Deep learning methods \cite{Li2019DeepLB,Domnich2021KappaMaskAC,Yan2018CloudAC,WIELAND2019111203,JEPPESEN2019247} use computer vision models (CNNs) and treat the cloud masking task as that of image segmentation tasks. CNNs have achieved superior performance thanks to their automatic feature extraction ability. A research paper published in 2019 \cite{JEPPESEN2019247} introduces Remote Sensing Network (RS-Net), which is a CNN architecture branched out of U-Net \cite{RFB15a} for cloud masking and was shown to achieve higher performance compared to the state-of-the-art rule-based approach known as Fmask \cite{Zhu2012ObjectbasedCA}. KappaMask \cite{Domnich2021KappaMaskAC} and MSCFF \cite{Li2019DeepLB} are two additional U-Net based CNN model that outperformed Fmask. All these models have reported their performances on several satellite images such as Sentinel-2, Landsat, etc., and also made use of human-annotated (some assisted by software) ground truth values (See in Table \ref{tab:datasets}). On the other hand, MLCommons Cloud Masking Benchmark operates on SLSTR images from the newer Sentinel-3 satellite, which uses Bayesian approach generated cloud masks as ground truth. The reference implementation provided by MLCommons Science Working Group achieved 92\% classification accuracy on the Sentinel-3 test set \cite{Thiyagalingam2022AIBF}.

The aforementioned deep learning approaches towards cloud masking are by no means exhaustive. If you know about other significant cloud masking or deep learning approaches, please inform us and we will add them here.''

\begin{table*}[htb]
    \centering
    \caption{This table lists several methods used for cloud masking with their respective dataset, ground truth, and performance \cite{las-2023-cloudmask-related}.}
    \label{tab:datasets}
    \resizebox{1.8\columnwidth}{!}{
    \begin{tabular}{|l|c|l|l|l|l|}
    \hline
        {\bf} & {\bf Reference} & {\bf Dataset} & {\bf Ground-truth} & {\bf Model}  & {\bf Accuracy} \\ \hline
        1 & \cite{Merchant2005ProbabilisticPB} & ATSR-2 & Human annotation & Bayesian screening & 0.917\\ \hline
        2 & \cite{WIELAND2019111203} & Sentinel-2 & Software-assisted human annotation (QGIS) & U-Net & 0.90 \\ \hline
        3 & \cite{WIELAND2019111203} & Landsat TM & Software-assisted human annotation (QGIS) & U-Net & 0.89 \\ \hline
        4 & \cite{WIELAND2019111203} & Landsat ETM+ & Software-assisted human annotation (QGIS) & U-Net & 0.89 \\ \hline
        5 & \cite{WIELAND2019111203} & Landsat OLI & Software-assisted human annotation (QGIS) & U-Net & 0.91 \\ \hline
        6 & \cite{Li2019DeepLB} & GaoFen-1 & Human annotation & MFFSNet & 0.98, mIOU = 0.87 \\ \hline
        7 & \cite{Domnich2021KappaMaskAC} & Sentinel 2 & Software-assisted human annotation (CVAT) & KappaMask & 0.91 \\ \hline
        8 & \cite{JEPPESEN2019247} & Landsat 8 Biome and SPARCS & Human annotation & RS-Net & 0.956 \\ \hline
    \end{tabular}}

\end{table*}


\section{Dataset} 

The dataset used in the benchmark is described in \cite{las-2023-cloudmask-related}: 
``MLCommons Cloud Masking Benchmark uses 180GB of satellite images from Sentinel-3 SLSTR (Level-1 processing, TOA Radiances and Brightness Temperature) satellite images. The dataset consists of 1070 images, captured during days and nights. The dataset also includes a cloud masking for each image, generated using Bayesian techniques. The reference implementation uses these cloud masks as ground truths for training and testing.

The dataset comes with the train-test split, where 970 images are used for training, and 100 images are used for testing. 
The images are of the dimension $1200 \times 1500$ with 15 different channels and 1 channel of Bayesian mask. Among the 15 channels, 3 channels are used to represent brightness, 6 channels are used to represent reflectance, and the remaining 6 channels are used to represent radiance. However, for the provided reference implementation, only a total of 10 channels, i.e., 6 channels of reflectance, 3 channels of brightness, and 1 channel of Bayesian mask are used as model inputs for training and testing.'' 

\begin{figure*}[htb]

\centering\includegraphics[width=0.75\textwidth]{images/cloudmask-preprocessing-training-data-2.pdf}
\caption{The preprocessing of the training data \cite{las-2023-cloudmask-related} .}
\label{fig:preprocessing-training}

\bigskip

\centering\includegraphics[width=0.75\textwidth]{images/cloudmask-preprocessing-testing-data-2.pdf}
\caption{The preprocessing of testing data \cite{las-2023-cloudmask-related}.}
\label{fig:preprocessing-testing}

\end{figure*}

\subsection{Data Loading and Preprocessing} \label{sec:preprocessing}

We quote from \cite{las-2023-cloudmask-related}:  ``For training data preprocessing, the images are first cropped from the dimension of $1200 \times 1500 \times 9$ to $1024 \times 1280 \times 9$ and then divided into 20 smaller-sized $256 \times 256 \times 9$ patches. After creating these patches out of each image in training set, we get a total of $19400$ patches for training. These patches are further split into training and validation set with $80/20$ split ratio, and then sent for training after shuffling. 

For the test dataset, the images are neither cropped nor shuffled. Instead, each test image are cut into 63 smaller patches of dimension $256 \times 256 \times 9$, by applying a horizontal and vertical stride of 176 pixels with zeros padding on the right and bottom edges of each image. We then get a total of $6300$ patches for entire test dataset. After getting the predictions from the model, these $256 \times 256 \times 1$ output patches (predicted cloud mask) are reconstructed to the size of $1200 \times 1500 \times 1$ and then evaluated with the Bayesian mask ground truth that has the same dimension. This preprocessing pipelines for training and testing are shown in Figure \ref{fig:preprocessing-training} and Figure \ref{fig:preprocessing-testing}.''

\subsection{Training}

We quote from \cite{las-2023-cloudmask-related} the training data methodolodgy:
``During training, the model takes a preprocessed patch of dimension $256 \times 256 \times 9$, and generates a cloud mask of dimension $256 \times 256 \times 1$. Once the cloud masks have been generated by the model during training, the accuracy is reported as the percentage of total pixels that are correctly classified compared to ground truth'' \cite{las-2023-cloudmask-related}. 

\subsection{Testing}

We quote from \cite{las-2023-cloudmask-related} the testing data methodology:
``During testing, the model generates a cloud mask of dimension $256 \times 256 \times 1$ for each $256 \times 256 \times 9$ patch. For each pixel in the image, the model outputs the probability of that pixel containing clear sky. Pixels that have a probability higher than 50\% are labeled as clear sky, and cloudy otherwise. Then, those patches are then reconstructed back to full-size masks of dimension $1200 \times 1500 \times 1$. 

The locations of the images used in the testing are depicted in Figure~\ref{fig:frames-inference} as well as their coordinate centers in Figure~\ref{fig:frames-dot}. As one can observe from the figures, most of the testing images are captured in the region of North Altantic Ocean and of the West Coast of Europe.
Furthermore, we display in Figure \ref{fig:frames-raw} the raw satellite images from the Sentinel-3 database that reflect the locations where the testing images are located. Figure \ref{fig:frames-mask} shows the cloud masks of the testing images.
The Table \ref{tab:inference} shows the individual attributes for the specific locations identified by a counter.''


\section{Model}

The MLCommons cloud masking benchmark \cite{Thiyagalingam2022AIBF} reference implementation uses the U-Net \cite{Ronneberger2015UNetCN} model. The architecture of U-Net is such that the model can output a label for each pixel in the image, instead of one label for the image. For this reason, the model first creates a contracting path to get deeper context and then has a symmetric expanding path to increase resolution that promotes localization of outputting a label.

\section{Logging}

Logging is an important tool for reproducing experiments and results. MLCommons uses a standard logging library called MLCommmons MLPerf/MLlog \cite{github-mlcommons-logging} to produce logs that contain information about each run. This is done so that the benchmarking submission can pass through their package checker to be accepted as a submission. It logs standard events such as "INTERVAL START", "INTERVAL END" and "POINT IN TIME" with key-value pairs. Hence, the logs generated from our experiments are one of the many artifacts submitted to MLCommons. 
However, the logs are not easily human readable. To allow easier-to-read logging information additional logs can be integrated for example with the cloudmesh StopWatch  (which is part of cloudmesh-common) \citep{cloudmesh-stopwatch} and provides in contrast to MLlog the ability to have multiple output formats including a human-readable summary table and explicitly named timers, and events.  Furthermore, cloudmesh StopWatch has an extension that automatically creates MLLog entries thus no additional effort is needed to augment the code to create MLLog entries. The development of log parsers was at the same time more sophisticated and simpler. Although this library was used extensively by the UVA team leading to easy-to-understand human readable logs, the NYU team elected to not utilize it requiring to run always a log parser instead of just looking into the log files for the results, making the overall inspection not as convenient as provided by the UVA code.

\section{Experiments}
\label{sec:exp}

In this section, we describe our submissions made to the cloud masking MLCommons Science Working Group. First, we describe the systems that we have used to make our submission in Section \ref{sec:hw}. After which we go through each step performed in the benchmarking in Sections \ref{sec:code}, \ref{sec:install}, and \ref{sec:parallel}.

\subsection{Compute Infrastructure}
\label{sec:hw}

This study was conducted on different compute infrastructures. These include: a desktop, the High Performance Computer (HPC) at UVA called {\em Rivanna}, and the HPC at NYU called {\em Greene}. We describe the computing resources in more detail and summarize some of their characteristics in Table \ref{tab:hwoverview}\footnote{The table represents the status of the machines in July 2023, it may since then have changed, but the studies are done with these resources.}.


\begin{table*}[htb]
    \caption{Overview of the computing resources used \cite{las23-cloudmask}, at the time of writing this paper.}
    \label{tab:hwoverview}
    \begin{center}
    \begin{tabular}{|l|r|r|r|r|r|r|r|}
        \hline
            {\bf Machine}  & {\bf Cores} & {\bf Memory} & {\bf GPU}   &   {\bf Memory} & {\bf GPUs} & {\bf Nodes}  & {\bf Commissioned} \\ 
                     &  {\bf /Node} & {\bf /Node}  &  {\bf Type}  & {\bf /GPU}     &   {\bf /Node}        & & \\
        \hline
        \hline
        Rivanna (UV)    & 128 & 2000GB   & A100 & 80GB &  8  & 10 & Feb 2022 \\
                        & 128 & 1000GB   & A100 & 40GB &  8  &  2 & Jun 2022 \\   
                        & 28  & 255GB    & K80  & 11GB &  8  &  8 & Jun 2018 \\
                        & 28  & 255GB    & P100 & 12GB &  4  &  4 & Jan 2018 \\
                        & 28  & 188GB    & V100 & 16GB &  4  &  1 & Feb 2019 \\
                        & 40  & 384GB    & V100 & 32GB &  4  & 12 & Feb 2021 \\
                        & 36  & 384GB    & V100 & 32GB &  4  &  2 & Apr 2022 \\
                        &  64 & 128GB     & RTX3090   & 24GB    & 4   &  5 & Feb 2023         \\
                        & 40  & 384GB    & RTX2080TI & 11GB & 10  &  2 & May 2021 \\
         \hline
         Greene (NYU)	& 48	& 384	& RTX8000 	& 48	& 4	& 73 & Nov 2020\\
	        & 48	& 384	& V100 	    & 32	& 4	& 10 & other dates \\
	        & 40	& 512	& V100 	    & 3     & 8 & 1  & not published \\
	        & 40	& 384	& V100 	    & 32    & 4	& 1  & \\
	        & 40	& 384	& V100 	    & 16    & 4	& 6  & \\
	        & 80	& 1024	& A100  8358	& 80    & 4 & 31 & \\
	        & 64	& 512	& A100  8380	& 80    & 4 & 9  & \\
%	        & 96	& 512	& MI50              & 32    & 8 & 20 & \\
%	        & 128	& 512	& MI100	            & 32    & 8 & 3  & \\
%	        & 128	& 512	& MI250	            & 64    & 8 & 2  & \\

         \hline
         Desktop 5950X     &  32 & 128GB     & RTX3090   & 24GB    & 1 & 1 & Feb 2022   \\
         \hline
    \end{tabular}
    \end{center}

\end{table*}

\paragraph{NYU Greene HPC} NYU Greene is a general-purpose cluster at New York University that supports a variety of job types and sizes. This includes jobs requiring multiple CPU cores, multiple GPU cards, or terabytes of memory. For this submission, we use V100 GPUs and RTX8000. More specifications of the cluster can be found on the NYU Greene HPC Homepage \cite{www-greene}.

\paragraph{UVA Rivanna HPC} The UVA machine is a cluster built on a condominium model where different groups add resources to the cluster to then be shared with its entire user base. As such the cluster is frequently updated and has due to its evolving updates a variety of GPUs. The various GPUs and their placement on the servers are listed in Table~\ref{tab:hwoverview}.

\paragraph{AMD 5090X RTX3090 Desktop} The desktop is a custom build desktop with fast NVMe and an RTX connected to PCIe4 with more details listed in Table~\ref{tab:hwoverview}. As the RTX3090 is built using the Ampere architecture from NVIDIA it performs similarly to an A100 when memory is not an issue. When using 32-bit it will be even faster. However, we have not utilized this for this application.


\subsection{Code modifications}
\label{sec:code}

The original code was provided by the Rutherford Appleton Laboratory (RAL) \cite{Thiyagalingam2022AIBF,github-laszewsk-mlcommons}. It also includes a change to the original accuracy calculation that was initially just based on a boolean but was corrected to use a numerical accuracy value and developed based on our observation by the RAL team and made available in early Dec. 2022. The reporting of results and additional timers needed for MLCommons were added by UVA and RAL and were available in July 2022 with minor updates since then. UVA developed for their version of the code the ability to customize the analysis of the runs through a portable Jupyter notebook that takes as input the FAIR \cite{www-fair} data produced by running the code via the convenient and easy-to-use cloudmesh experiment executor \cite{github-cloudmesh-ee,las-2023-escience-cloudmask,las-2023-ai-workflow}. This system not only allows to change the epochs but any hyperparameter significant for this application. However, for this study, we restricted the search parameters to just epochs to evaluate results reported by the NYU Greene experiments.

\paragraph{Modification of the code at NYU} The team at NYU cloned the open-source code \cite{www-mlcommons-science-github} repo of the MLCommons' cloud masking benchmark and modified their version as follows. 
Based on the finding that the accuracy value does not change while increasing the number of epochs, this was reported to the UVA and RAL team. They worked on releasing a new version in early Dec. 2022 that we then used. We added the Early Stopping \cite{Caruana2000OverfittingIN} functionality to avoid overfitting and save the model with the best weights that has generalizability. And further controlled for randomness by setting the seed for Python, TensorFlow, numpy, and the random. A batch script was developed that allows modification of different runs with different epochs. This allowed us to run more than one experiment and report the variance in the performance with more confidence.


\subsection{Installation}
\label{sec:install}

\paragraph{Installation on UVA Rivanna} UVA created an updated installation instruction that is the basis for porting to other machines and has been basis for the NYU work. Although the UVA code was first deployed using bare metal software, the UVA team changed it to use singularity in order stay up to date with the NVIDIA most recent software releases that are recommended to be deployed via the containers provided by NVIDIA. This change makes overall deployment easier. In principal this deployment can be replicated on any machine that supports container frameworks such as singularity.

\paragraph{Installation on NYU HPC Greene} The NYU team provided updates to the specific documentation for NYU Greene using bare metal deployment of the cloud masking. This includes code to reflect the system-specific features of Greene that were available before July 2023. We worked on installing and running the reference implementation provided by MLCommons on HPC Greene. This was done to make sure the reference implementation can be successfully replicated on NYU Greene. In this process, the provided instructions to support installing the right packages with their right versions, as well as the Python environment on NYU HPC Greene as the machine used different bare metal software versions in comparison to the UVA machine.  In contrast to UVA which has switched to singularity, the NYU team uses tensorflow and python versions that were provided by the system staff in bare metal. The instructions were integrated into the MLCommons Development repository. Although Greene supports singularity, the NYU team decided to use the bare metal deployment. However, as of July 2023, this deployment could no longer be replicated as the underlying NVIDIA libraries were changed and the team graduated so the documentation is not kept up-to-date. 
In an independent work conducted by Ruochen Gu, a version for Greene was developed that uses also singularity with overlays. However, as accounts on Greene expired this code is no longer maintained, but could be used by Greene users with appropriate updates.

\paragraph{Portability} Groups that would like to port it to other machines may get inspired either by the bare metal deployment instructions or by the UVA singularity deployment using images directly distributed by NVIDIA for tensorflow \cite{las-2023-escience-cloudmask}. The code from UVA does not use overlays but is more sophisticated and allows hyperparameter searches leveraging batch and non-batch compute resources.

\paragraph{Reproducability} The code developed by UVA and especialy the use of the cloudmesh experimet executor \cite{github-cloudmesh-ee,las-2023-escience-cloudmask} allows for sophisticated result logging promoting the FAIR principle.

\subsection{Parallelization}
\label{sec:parallel}


\paragraph{Parallelization at NYU Greene} The experiments were conducted in parallel fashion to utilize the HPC system that is being used. We reused the Cloudmesh YAML configuration contributed by the UVA team 
that allows the use of YAML files to initialize the parameters of the main code.
We integrated this logic into the SLURM script to generate such YAML files 
on the fly, such that one file was sufficient to run all the given jobs in parallel, without having to run these jobs separately. We also added the functionality for saving different models in different directories while the jobs are running in parallel given with the same hyperparameters, such that no model overwriting can take place. These changes have been integrated in the code version from NYU along with the results to the MLCommons benchmark.


\paragraph{Parallelization at UVA Rivanna} As we use a High Performance Computer (HPC) we can conduct hyperparameter searches in parallel while leveraging the convenient cloudmesh experiment executor (ee)  hyperparameter permutation workflow tool \cite{las-2023-ai-workflow,github-cloudmesh-ee,las-2023-mlcommons-edu-eq} that specializes to create reproducible runs and results simplifying the implementation of the FAIR principle. This tool is also used for other scientific applications. The software has been successfully utilized to manage long-running hyperparameter searches for earthquake forecasting. As this tool and its configuration files take care of utilizing separate directories for running the code and producing the output, no modification to the original code was needed other than switching to the updated configuration files. 

As in contrast to the NYU code the cloudmesh-ee allows the specification of ranges of parameters in the YAML file a single command line can be used on a single YAML file to generate an extensive hyperparameter search without modification of the program. Thus instead of spending a significant amount of time generating various YAML files, this is accomplished in seconds through cloudmesh-ee for all hyperparameters and not just a single one (e.g. epoch).

Cloudmesh experiment executor is based on an easy-to-use but sophisticated enhancement to batch queuing script management that we can apply to SSH, SLURM~\cite{www-slurm}, and LSF~\cite{www-lsf}. To control the permutations a simple configuration parameter file is specified in YAML format~\cite{github-cloudmesh-ee} and started in a one-line command. 
When executing, it creates for each experiment a unique directory, in which the code and configuration parameters are stored. This allows the reproduction of a single experiment from within this folder given a unique set of configuration parameters. When the experiment is executed, all output files are stored in the directory. Thus it promotes the implementation of the FAIR principle as input and output files are available.
This provides a more integrated, but yet more flexible approach than traditional shell scripts. More importantly, the submission of the generated experiments can be conducted with a single online terminal command. 
Furthermore each experiment is submitted as its own job and thus projects smaller tasks that fit better in the boundaries of the queing system policies we encountered. Submission in parallal has the advantage that the overall wait time in the queue is reduced as at UVA it takes longer to obtain more resources with time-consuming requests. Lastly, we evaluated the usability of cloudmesh-ee with an undergraduate. It was possible for the student not familiar with the cloud masking benchmark to run the benchmark within one day \cite{las-2023-escience-cloudmask} using cloudmesh-ee.
The reasons why the UVA team chose cloudmesh-e is not only because it was available before this effort started, but because of additional reasons listed in Table~\ref{tab:advantage-ee}, where green highlighted areas show which system has a perceived advantage and white fields show no advantage. At NYU the educational experience of recreating a small portion of the cloudmesh-ee framework stood in the foreground. The NYU team had the choice to reuse cloudmesh-ee or to develop an alternative approach.
In addition, we can leverage the cloudmesh compute coordination framework (cloudmesh-cc) that can submit the hyperparameter searches onto a heterogeneous set of HPC machines in parallel \cite{las-2023-ai-workflow,github-cloudmesh-cc,las22-cloudmesh-cc-reu}. The tool creates first a workflow for executing all hyperparameter experiments. 


\newcommand{\OK}{\cellcolor{green!10}}
\newcommand{\GR}{\cellcolor{gray!10}}

\begin{table}[p]

\caption{Comparison between the two approaches to run the benchmark. NYU chose to use a batch script based approach, while UVA used cloudmesh-ee}\label{tab:advantage-ee}

\centering
 \resizebox{0.99\columnwidth}{!}{%
\begin{tabular}{|p{3cm}||p{3cm}|p{3cm}|}
    \hline
     \rowcolor{gray!10}{\bf Comparison}             & {\bf Batch script} &  {\bf cloudmesh-ee} \\
     \hline
     \hline
     \GR Installation & \OK Easy as distributed with code as bash script & \OK Easy as distriuted with code thorugh pip install\\
     \hline
     \GR Submission & \OK sh script with loop & \OK sh script that contains every job to be submitted \\
     \hline
     \GR Queing System & SLURM & \OK SLURM, LSF, ssh \\
     \hline
     \GR File Management  & Files and directories are hardcoded and are not visible in the yaml file without inspecting the code & \OK Directories and files are created relative to the scirpt execution directory thus allowing FAIR. Their names are automatically derived from the combination of hyperparameters specified  \\
     \hline
     \GR Program copy & A program copy is not generated & \OK For each experiment its own program copy is generated manifasting the FAIR principle as the program becomes part of the metadata \\
     \hline
     \GR Parameter specification & For each experiment its own yaml file needs to be generated by hand to modify other hyperparameters than epochs & \OK A single yaml file can be used to create many experiments allowing a hyperparameter grid search\\
     \hline
     \GR Direct parameter search & only epochs & \OK all parameters incl. epoch are supported\\
     \hline
     \GR Experiments can be repeated easily without overwriting old results & No & \OK yes \\
     \hline
     \GR Experiments form different computers can easily be merged & No & \OK Yes \\
     \hline
     \GR Single experiments can easily be rerun in case of failure & No & \OK Yes \\
     \hline
     \GR Logging & Based on MLPerf logging and is not easily human readable & \OK Based on MLPerf Logging but is automatically created from named timers and events \\ 
     \hline
    \GR Has been used for other MLCommons applications & No & \OK Yes \\
    \hline
    \GR Was previously available and is not new development & No & \OK Yes \\
    \hline
    \GR Learning curve & High as file management difficult to understand for graduate and undergraduates, and due to the need to create for each adjustable parameter its on YAML file & \OK Easy, proven to be usable for an undergraduate in less then an hour, due to only one YAML file needed for all experiements and the separation of the runtime environment for each experiment. \\
    \hline
    %\GR Perceived value in $\star$s  & $\star\star$ & \OK $\star\star\star\star\star\star\star\star\star\star\star\star$ \\
    %\hline
\end{tabular}
}

\end{table}

\subsection{Results} 

Using the two different systems we now report selected performance (time per epoch) and the scientific metrics (accuracy) while focusing mostly on NYU Greene.

\paragraph{Results from NYU Greene} We train the model for a total of 200 epochs with early stopping \cite{Caruana2000OverfittingIN} with a patience of 25. We chose 25 as the patience since the loss curve for the validation set was very bumpy as shown in Figure \ref{fig:Validation loss}. The bumpy training as well as the difficulty of training this model is suspected to be due to the usage of Bayesian Masks as the ground truth, which has been first observed in \cite{Thiyagalingam2022AIBF}. We use a learning rate of $1e-3$ with a batch size of $32$. The remaining hyper parameters such as the crop size, patch size, train test split, etc., are kept the same as those of the reference implementation. We ran each of our experiments $5$ times and report the average for tarining and inference times for the model inputs.


Table \ref{tab: Results} shows the results of our best model on both V100 and RTX8000 GPUs. Figure \ref{fig:Training loss} shows the training loss for the five runs on the same seed, and we see that run 2 performs the best with the least loss. We observed for this benchmark the best model weights at epoch 147, and an accuracy of 0.896 while using early stopping. The train and validation loss curve for run 2 is shown in Figure \ref{fig:Losses of Run 2}. The average inference accuracy achieved in 5 runs is 0.889.

\paragraph{Comparative Results from UVA} While the results from NYU have been created using a shell script, the results from UVA have been created with the convenient cloudmesh-ee \cite{las-2023-ai-workflow,github-cloudmesh-ee}. We used cloudmesh-ee to create all results on UVA's Rivanna HPC. We added results obtained from NYU's Greene and show the comparative time consumption for a run with one epoch in Figure \ref{fig:epoch-compare}. We observe that the fastest machines are an NVIDIA station using the build in raid followed by a custom build desktop with fast NVMe. 
The second fastest machine is a custom-designed desktop with an RTX3090 and fast NVMe storage.
The third fastest machine is an NVIDIA DGX node that is built into Rivanna while using the local NVMe storage. 
Both Rivanna and Greene utilize a shared file storage system that slows down the capabilities of the GPU cards while not being able to keep up with the calculations. This is shown in Figure \ref{fig:epoch-compare} as all machines using non-local storage perform significantly slower.
The Rivanna K80 performs the worst in our comparison due to its older hardware.
We also observed a large amount of variations as the server may be used in multi-user mode (indicated by the black lines). To guarantee no disturbance by others it is recommended to use exclusive server nodes. However, variations to access data on the shared file system can not be isolated.

\paragraph{Availability} The results are publicly available at \cite{www-mlcommons-cloudmask-results}.


\begin{figure}[htb]
\centering\includegraphics[width=1.0\columnwidth]{images/epoch_vs_loss (2).pdf}
\caption{Training and Validation Loss of Run 2 on NYU Greene.}
\label{fig:Losses of Run 2}
\end{figure}

%This model is run on xyx gpu. We need to add the best model for the other GPU as well, experiments yet to be run.


\begin{table*}[thb]
    \centering
    \caption{This table shows the results. Here the time for training and inference is reported as the avg. time per epoch for both training and inference.}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
        Epochs & Seed & Train acc. & Test acc.  & avg. train time  & avg. inference time & GPU & Machine\\ \hline
        200 & 1234 & 0.895950 & 0.884168 & 142.47 & 1.73 & v100 & Greene \\ \hline
        147 & 1234 & 0.908647 & 0.896051 & 98.75 & 1.46 & v100 & Greene\\ \hline
        162 & 1234 & 0.897208 & 0.890086 & 108.16 & 1.44 & v100 & Greene\\ \hline
        200 & 1234 & 0.897057 & 0.889887 & 126.44 & 1.53 & v100 & Greene\\ \hline
        183 & 1234 & 0.896541 & 0.885192 & 121.21 & 1.41 & v100 & Greene \\ \hline
    \end{tabular}
    \label{tab: Results}
\end{table*}

%Still need to add results from Rivanna, and the desktop if Professor Gregor has run that.

\begin{figure}[htb]
\centering\includegraphics[width=1.0\columnwidth]{images/training_loss_diff_runs.pdf}
\caption{Training Loss on NYU Greene with different runs with the same seed. With Early Stopping and patience of 25, the 5 different runs stop their training and save the model weights at epochs 200, 147, 162, 200, 183, respectively.}
\label{fig:Training loss}
\end{figure}

\begin{figure}[htb]
\centering\includegraphics[width=1.0\columnwidth]{images/validation_loss_diff_runs.pdf}
\caption{Validation Loss with different runs with the same seed on on NYU Greene.}
\label{fig:Validation loss}
\end{figure}

\begin{figure}[htb]
\centering\includegraphics[width=1.0\columnwidth]{images/validation_accuracy_diff_runs.pdf}
\caption{Validation Accuracy with different runs with the same seed on NYU Greene.}
\label{fig:Validation Accuracy}
\end{figure}

\begin{figure}[htb]
\centering\includegraphics[width=1.0\columnwidth]{images/training_accuracy_diff_runs.pdf}
\caption{Training Accuracy with different runs with the same seed on NYU Greene.}
\label{fig:Training Accuracy}
\end{figure}

\begin{figure}[ht]
\centering\includegraphics[width=1.0\columnwidth]{images/epoch_vs_accuracy (2).pdf}
\caption{Training and Validation Accuracy of Run 2 on NYU Greene.}
\label{fig:Accuracies of Run 2}
\end{figure}



\begin{figure}[ht]
\centering\includegraphics[width=1.0\columnwidth]{images/gregor-epoch-1.pdf}
\caption{Comparison of runtimes of cloud masking for 1 epoch on various GPUs on Rivanna, Greene, a dgx-station (a100-dgx), and the desktop \cite{las-2023-escience-cloudmask}.}
\label{fig:epoch-compare}
\end{figure}


\paragraph{Random seed on HPC systems} During the entire process of benchmarking, i.e., going from installation to logging capabilities, to adding early stopping, and leveraging paralelization based on experiment submissions, we came across the obstacle of randomness. Given the same seed for all runs that controls any randomness added whatsoever, we still notice variation in the experiments. It is important to note that this is a documented feature when using GPUs. To be extra careful we have provided additional seeds initializations for various Python libraries as documented in an earlier section. However, it would be useful that a future activity investigate further the impact of the various seeds in a parallel application on the random numbers.

\section{Conclusion}

In this paper, we describe our submission to MLCommons Science cloud masking benchmark on two systems: NYU Greene and discuss differences how the UVA team produced their results. We provide documented instructions on how to run the reference implementation on both systems for this benchmark.\footnote{Disclaimer: the documentation from NYU no longer works due to a system upgrade at NYU. A transition to singularity from the NYU system staff was recommended. This has been independently worked on by Ruochen Gu. However, as the accounts on Greene expired, this work could not be completed and is theefore not documented in this paper. For availability of portable code please contact G. von Laszewski.} 
The NYU team contributed a modification to the code that integrates early stopping. A bash script was developed that replicates some features provided by cloudmesh-ee while the team explored in more depth how to use queuing systems job arrays feature with SLURM. We report the best performance on the NYU system, and also the average time taken for an epoch for training and inference on a variety of machines. The code is available in the MLCommons Science benchmark's GitHub repository.


% conlusion can not have refernces with all the artifacts mentioned in Section \ref{sec:exp}.

\begin{acks}

  Work was in part funded by (a) NIST 60NANB21D151T (b) NSF
  CyberTraining: CIC: CyberTraining for Students and Technologies from
  Generation Z with the award numbers 1829704 and 2200409, and (c)
  Department of Energy under the grant Award No. DE-SC0023452 (d) NSF
  Collaborative Research: Framework: Software: CINES: A Scalable
  Cyberinfrastructure for Sustained Innovation in Network Engineering
  and Science with the award number 2210266. The work from the UVA
  team was conducted at the Biocomplexity Institute at
  the University of Virginia.  We like to thank the NYU AIFSR team for
  their contributions. We especially like to thank Ruochen Gu who
  continued to work on this project on voluntary basis.

\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{vonLaszewski-cloudmask-related}

%%
%% If your work has an appendix, this is the place to put it.


\section*{Contributions}

The NYU team worked under AI for Scientific Research, a VIP group at NYU, where the team was composed of a coordinator {\em LM} , machine learning developer {\em VC}, and a researcher {\em RG}. The team was under the technical and academic supervision of {\em SS}. {\em VC} was the teamâ€™s machine learning developer who worked on setting up and experimenting on the cloud masking application through NYU HPC, as well as developing major part of the code. {\em RG} was the researcher of the group and was responsible for researching in the area of cloud masking and AI benchmarking and bringing domain expertise to the group. {\em LM}  worked as the coordinator of the NYU group and was responsible for managing tasks and priorities of the project. All members of the team contributed to the code and the paper. 

{\em GvL} was the technical lead  at UVA and contributed the cloudmesh-ee workflow code, the cloudmesh StopWatch (which is part of cloudmesh-common) and integrated the cloudmesh timers into the code. He ran all benchmarks on Rivanna and the Desktop.  He  facilitates the submission to MLCommons. {\em GvL}, {\em GCF}, and {\em JP} provided general direction for the project including  feedback on progress of code development as well as benchmarking. {\em GvL} contributed to code development by the NYU team, the paper, and facilitated many hackathons with the NYU team. {\em SJ} and {\em JP} provided the  original cloud masking implementation and developed the new accuracy value to which {\em GvL} contributed. For UVA {\em GvL} also developed workflow-related code to integrate VPN authentication, GPU energy monitoring, the initial documentation to port the code on other machines, and the development of a reusable singularity container for the application. Through using cloudmesh-ee a portable code using the FAIR principle is available buy UVA.

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.

